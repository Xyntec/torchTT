{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Basic linear algebra in torchTT\n",
    "</p>\n",
    "This notebook is an introduction into the basic linar algebra operations that can be perfromed using the torchtt package.\n",
    "The basic operations such as +,-,*,@,norm,dot product can be performed between torchtt.TT instances without computing the full format by computing the TT cores of the result.\n",
    "One exception is the elementwise division between TT objects. For this, no explicit form of the resulting TT cores can be derived and therefore optimization techniques have to be employed (see the notebook fast_tt_operations.ipynb).\n",
    "\n",
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tn\n",
    "try:\n",
    "    import torchtt as tntt\n",
    "except:\n",
    "    print('Installing torchTT...')\n",
    "    %pip install git+https://github.com/ion-g-ion/torchTT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a couple of tensors for the opperations that follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = [10,10,10,10]\n",
    "o = tntt.ones(N)\n",
    "x = tntt.randn(N,[1,4,4,4,1])\n",
    "y = tntt.TT(tn.reshape(tn.arange(N[0]*N[1]*N[2]*N[3], dtype = tn.float64),N))\n",
    "A = tntt.randn([(n,n) for n in N],[1,2,3,4,1])\n",
    "B = tntt.randn([(n,n) for n in N],[1,2,3,4,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addition\n",
    "\n",
    "The TT class has the \"+\" operator implemeted. It performs the addition between TT objects (must have compatible shape and type) and it returns a TT object. \n",
    "One can also add scalars to a TT object (float/int/torch.tensor with 1d).\n",
    "\n",
    "The TT rank of the result is the sum of the ranks of the inputs. This is usually an overshoot and rounding can decrease the rank while maintaining the accuracy.\n",
    "\n",
    "Here are a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x+y \n",
    "print(z)\n",
    "# adding scalars is also possible\n",
    "z = 1+x+1.0\n",
    "z = z+tn.tensor(1.0)\n",
    "# it works for the TT amtrices too\n",
    "M = A+A+1 \n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtraction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplication (elementwise)\n",
    "\n",
    "One can perform the elementwise multiplication $\\mathsf{u}_{i_1...i_d} = \\mathsf{x}_{i_1...i_d} \\mathsf{y}_{i_1...i_d}$ between 2 tensors in the TT format without goin to full format.\n",
    "The main issues of this is that the rank of the result is the product of the ranks of the input TT tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = x*y\n",
    "print(u)\n",
    "\n",
    "M2 = A*A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix vector product and matrix matrix product\n",
    "\n",
    "* TT matrix and TT tensor: $(\\mathsf{Ax})_{i_1...i_d} = \\sum\\limits_{j_1...j_d}\\mathsf{A}_{i_1...i_d,j_1...j_d} \\mathsf{x}_{j_1...j_d}$\n",
    "* TT matrix and TT matrix: $(\\mathsf{AB})_{i_1...i_d,k_1...k_d} = \\sum\\limits_{j_1...j_d}\\mathsf{A}_{i_1...i_d,j_1...j_d} \\mathsf{B}_{j_1...j_d,k_1...k_d}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplication can be performed between a TT operator and a full tensor (in torch.tensor format) the result in this case is a full tn.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A@tn.rand(A.N, dtype = tn.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kronecker product\n",
    "\n",
    "\n",
    "For computing the Kronecker product one can either use the \"**\" operator or the method torchtt.kron()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x**y)\n",
    "print(A**A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norm\n",
    "\n",
    "Frobenius norm of a tensor $||\\mathsf{x}||_F^2 = \\sum\\limits_{i_1,...,i_d} \\mathsf{x}_{i_1...i_d}$ can be directly domputed from a TT decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.norm())\n",
    "print(A.norm())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot product and summing along modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df6fc3a9b7a9c6f4b0308ab6eb361a4cabbf6b5db181383d07014ff4304e5cb3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
